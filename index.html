<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>On-Device Transcriber (EN/HI) — Browser Only</title>
  <style>
    :root{
      --ink:#0f172a; --muted:#64748b; --bg:#0b1020; --card:#0f162c; --chip:#162043; --accent:#6ee7b7;
      --ring:rgba(110,231,183,.25);
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      color:#e5e7eb; background: radial-gradient(1200px 800px at 20% -10%, #122245 0%, #0b1020 60%);
      -webkit-font-smoothing:antialiased; line-height:1.5;
    }
    .wrap{max-width:920px;margin:40px auto;padding:20px}
    .title{font-size:clamp(1.4rem, 1rem + 2vw, 2.1rem);font-weight:800;letter-spacing:.3px}
    .subtitle{color:var(--muted);margin-top:6px;font-size:.98rem}
    .card{
      background:linear-gradient(180deg, rgba(255,255,255,.04), rgba(255,255,255,.02));
      border:1px solid rgba(255,255,255,.07);
      border-radius:16px; padding:16px; box-shadow:0 10px 30px rgba(0,0,0,.25);
    }
    .row{display:flex;gap:14px;flex-wrap:wrap;align-items:center}
    .pill{
      background:var(--chip); border:1px solid rgba(255,255,255,.08); padding:10px 12px;
      border-radius:999px; display:inline-flex; gap:8px; align-items:center; color:#d1d5db;
      font-size:.9rem;
    }
    input[type="file"]{
      display:none;
    }
    .btn, label[for="file"]{
      cursor:pointer; background:#111a33; border:1px solid rgba(255,255,255,.10);
      padding:10px 14px; border-radius:10px; color:#e5e7eb; font-weight:600;
      transition:transform .04s ease, border-color .2s ease, box-shadow .2s ease;
      user-select:none;
    }
    .btn:hover, label[for="file"]:hover{
      border-color:rgba(255,255,255,.22); box-shadow:0 0 0 6px var(--ring);
    }
    .btn:active, label[for="file"]:active{ transform:translateY(1px) }
    select, .toggle{
      background:#0f162c; color:#e5e7eb; border:1px solid rgba(255,255,255,.12);
      border-radius:10px; padding:10px 12px; font-weight:600;
    }
    .progress{
      height:10px; background:#0c1430; border-radius:999px; overflow:hidden; border:1px solid rgba(255,255,255,.08);
    }
    .bar{
      height:100%; width:0%; background:linear-gradient(90deg, var(--accent), #22d3ee);
      transition:width .2s ease;
    }
    textarea{
      width:100%; min-height:260px; resize:vertical; border-radius:12px; padding:14px;
      color:#e5e7eb; background:#0b1227; border:1px solid rgba(255,255,255,.12);
      font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; font-size:.98rem;
      line-height:1.6;
    }
    .muted{color:var(--muted); font-size:.9rem}
    .footer{margin:22px 0 6px; color:#90a3b4; font-size:.85rem}
    .chip{display:inline-flex;align-items:center;gap:8px;padding:6px 10px;border-radius:999px;background:#0e1836;border:1px solid rgba(255,255,255,.08)}
    .dot{width:8px;height:8px;border-radius:50%}
    .ok{background:var(--accent)}
    .warn{background:#fbbf24}
    .err{background:#f87171}
    .grid{display:grid;grid-template-columns:1fr;gap:14px}
    @media (min-width:880px){ .grid{grid-template-columns: 1.1fr .9fr} }
    .small{font-size:.85rem}
    .mono{font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;}
  </style>
</head>
<body>
  <div class="wrap">
    <div class="title">On-Device Transcriber (English/Hindi)</div>
    <div class="subtitle">Runs fully in your browser using a tiny Whisper model. Works with audio or video files. No server, no API, no downloads.</div>

    <div class="grid" style="margin-top:18px">
      <div class="card">
        <div class="row" style="justify-content:space-between; align-items:flex-start">
          <div class="row">
            <label for="file" class="btn">Choose audio/video</label>
            <input id="file" type="file" accept="audio/*,video/*" />
            <button id="transcribe" class="btn">Transcribe</button>
            <button id="copy" class="btn" title="Copy transcript">Copy</button>
            <button id="clear" class="btn" title="Clear">Clear</button>
          </div>
          <div class="row">
            <span class="pill">
              <span>Language</span>
              <select id="language">
                <option value="auto" selected>Auto-detect</option>
                <option value="en">English</option>
                <option value="hi">Hindi</option>
              </select>
            </span>
            <span class="pill">
              <span>Model</span>
              <select id="model">
                <option value="Xenova/whisper-tiny" selected>tiny (fast, multilingual)</option>
                <option value="Xenova/whisper-small">small (better, slower)</option>
              </select>
            </span>
          </div>
        </div>

        <div style="margin:14px 0 8px" class="muted small">
          Tip: For long videos, start with <b>tiny</b>. If accuracy is low, switch to <b>small</b>.
        </div>

        <div class="progress" aria-label="progress" style="margin:12px 0 10px">
          <div id="bar" class="bar"></div>
        </div>

        <div class="row" style="gap:10px;margin:6px 0 14px">
          <span class="chip"><span class="dot" id="engine-dot"></span> <span id="engine-text">Engine: not loaded</span></span>
          <span class="chip"><span class="dot ok"></span> Client-side only</span>
          <span class="chip"><span class="dot ok"></span> No API / No upload</span>
        </div>

        <textarea id="out" placeholder="Transcript will appear here…"></textarea>

        <div class="footer">
          Supported types depend on your browser (e.g., MP3, WAV, M4A, OGG, WEBM, MP4 video audio track).
          If a video doesn’t decode, convert it client-side before upload or try a different browser.
        </div>
      </div>

      <div class="card" style="padding:16px">
        <div style="font-weight:700;margin-bottom:6px">How it works</div>
        <ol class="small" style="margin:0 0 10px 18px">
          <li>Loads a Whisper model into the browser (WebGPU/WebAssembly).</li>
          <li>Decodes your file locally with the Web Audio API.</li>
          <li>Runs transcription fully offline (no server requests).</li>
        </ol>
        <div class="muted small">
          Notes:
          <ul style="margin:8px 0 0 18px">
            <li><b>Auto-detect</b> language is default; choose <b>Hindi</b> for better Devanagari output.</li>
            <li>Larger models are slower but more accurate.</li>
            <li>Keep this tab open during processing; progress shows model load & transcription steps.</li>
          </ul>
        </div>
        <div class="muted small" style="margin-top:10px">
          Privacy: Your files never leave your device. Model files are fetched from a public CDN.
        </div>
      </div>
    </div>
  </div>

  <!-- Transformers.js (runs Whisper in-browser) -->
  <script type="module">
    import { pipeline, env } from "https://cdn.jsdelivr.net/npm/@xenova/transformers@3.0.0/dist/transformers.min.js";

    // Prefer WebGPU if available (faster). Falls back to WASM.
    env.backends.onnx.wasm.wasmPaths = "https://cdn.jsdelivr.net/npm/onnxruntime-web@1.20.0/dist/";
    env.allowLocalModels = false; // load models from Hugging Face CDN
    env.useBrowserCache = true;   // cache models in the browser for faster reloads

    const fileEl = document.getElementById('file');
    const outEl = document.getElementById('out');
    const barEl = document.getElementById('bar');
    const transcribeBtn = document.getElementById('transcribe');
    const copyBtn = document.getElementById('copy');
    const clearBtn = document.getElementById('clear');
    const langEl = document.getElementById('language');
    const modelEl = document.getElementById('model');
    const engineDot = document.getElementById('engine-dot');
    const engineText = document.getElementById('engine-text');

    // UI helpers
    function setProgress(x){ barEl.style.width = Math.max(0, Math.min(100, x)) + '%'; }
    function setEngine(status, color='ok'){
      engineDot.className = 'dot ' + color;
      engineText.textContent = 'Engine: ' + status;
    }
    function lockUI(lock){
      [fileEl, transcribeBtn, langEl, modelEl, clearBtn].forEach(el => el.disabled = lock);
      transcribeBtn.textContent = lock ? 'Transcribing…' : 'Transcribe';
    }

    // Keep a single transcriber instance per selected model (cached)
    const transcribers = new Map();

    async function getTranscriber(modelId){
      if (transcribers.has(modelId)) return transcribers.get(modelId);
      setEngine('loading model…');
      const transcriber = await pipeline('automatic-speech-recognition', modelId, {
        progress_callback: (p) => {
          // p: { status, file, loaded, total }
          if (p.status === 'initiate') setProgress(5);
          else if (p.status === 'download') {
            const pct = 5 + Math.round((p.loaded / (p.total || 1)) * 60);
            setProgress(pct);
          } else if (p.status === 'load') setProgress(70);
        }
      });
      transcribers.set(modelId, transcriber);
      setEngine('ready');
      return transcriber;
    }

    // Decode audio from any audio/video file using Web Audio
    async function decodeToPCM(file){
      const arrayBuf = await file.arrayBuffer();
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const audioBuf = await audioCtx.decodeAudioData(arrayBuf);
      // Downmix to mono Float32
      const ch = audioBuf.numberOfChannels;
      const len = audioBuf.length;
      const data = new Float32Array(len);
      const tmp = new Float32Array(len);
      for (let i = 0; i < ch; i++){
        audioBuf.getChannelData(i, tmp);
        for (let j = 0; j < len; j++) data[j] += tmp[j] / ch;
      }
      return { samples: data, sampleRate: audioBuf.sampleRate };
    }

    // Simple resampler to 16000 Hz using OfflineAudioContext (better quality than naive)
    async function resampleTo16k({ samples, sampleRate }){
      if (sampleRate === 16000) return samples;
      const src = new AudioBuffer({ length: samples.length, sampleRate, numberOfChannels: 1 });
      src.copyToChannel(samples, 0);
      const duration = samples.length / sampleRate;
      const targetLength = Math.ceil(16000 * duration);
      const off = new OfflineAudioContext(1, targetLength, 16000);
      const bufSrc = new AudioBufferSourceNode(off, { buffer: src });
      bufSrc.connect(off.destination);
      bufSrc.start();
      const rendered = await off.startRendering();
      const out = new Float32Array(rendered.length);
      rendered.copyFromChannel(out, 0);
      return out;
    }

    async function transcribe(){
      if (!fileEl.files || fileEl.files.length === 0){
        alert('Please choose an audio/video file first.');
        return;
      }
      lockUI(true);
      outEl.value = '';
      setProgress(0);
      setEngine('preparing…','warn');

      try{
        const file = fileEl.files[0];
        // 1) Model
        const modelId = modelEl.value; // 'Xenova/whisper-tiny' or 'Xenova/whisper-small'
        const asr = await getTranscriber(modelId);

        // 2) Decode -> PCM -> resample
        setEngine('decoding media…','warn'); setProgress(75);
        const pcm = await decodeToPCM(file);
        const mono16k = await resampleTo16k(pcm);

        // 3) Transcribe
        setEngine('transcribing…','warn'); setProgress(80);

        // Language handling: 'auto' lets Whisper detect.
        const langSel = langEl.value;
        const language = (langSel === 'auto') ? undefined : langSel;

        // Chunking large audio to keep memory sane (approx 30s chunks at 16kHz)
        const sr = 16000;
        const chunkSec = 30;
        const chunkSize = sr * chunkSec;
        let text = '';
        for (let start = 0; start < mono16k.length; start += chunkSize){
          const end = Math.min(mono16k.length, start + chunkSize);
          const chunk = mono16k.subarray(start, end);

          const result = await asr(chunk, {
            // Force transcription (not translation)
            // NOTE: Whisper will output Devanagari for Hindi when language='hi'
            // For auto, it attempts detection.
            // @ts-ignore
            chunk_length_s: chunkSec,
            return_timestamps: false,
            language,
          });

          text += (result.text || '') + '\n';
          const pct = 80 + Math.round(20 * (end / mono16k.length));
          setProgress(Math.min(99, pct));
        }

        outEl.value = text.trim();
        setEngine('done');
        setProgress(100);
      }catch(err){
        console.error(err);
        setEngine('error','err');
        alert('Could not transcribe this file in the browser. Try another format or smaller model.');
      }finally{
        lockUI(false);
      }
    }

    transcribeBtn.addEventListener('click', transcribe);
    copyBtn.addEventListener('click', async ()=>{
      try{
        await navigator.clipboard.writeText(outEl.value || '');
        copyBtn.textContent = 'Copied!';
        setTimeout(()=> copyBtn.textContent = 'Copy', 900);
      }catch{}
    });
    clearBtn.addEventListener('click', ()=>{
      outEl.value = '';
      fileEl.value = '';
      setProgress(0);
      setEngine('ready');
    });

    // Detect WebGPU availability to inform user (optional)
    (async ()=>{
      const hasWebGPU = !!navigator.gpu;
      setEngine(hasWebGPU ? 'ready (WebGPU available)' : 'ready (WASM fallback)');
    })();
  </script>
</body>
</html>
